{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fcba1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "#Machine learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3d17c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "############## DIALECT IDENTIFICATION ################\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3859d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "datasets = []\n",
    "labels = [\"HongKong\", \"Philippines\", \"Singapore\", \"Canada\", \"India\"]\n",
    "for dialect in range(len(labels)):\n",
    "    comb=None\n",
    "    for i in range(300):\n",
    "        with open(\"..\\CleanCorpora\\{0}\\_{0}CorporaCombined{1}.txt\".format(labels[dialect], i), \"r\", encoding='UTF-8') as file:\n",
    "            text = file.read()\n",
    "        df = pd.DataFrame({\"Label\": labels[dialect], \"Document\": [text]})\n",
    "        dfs.append(df)\n",
    "    comb = pd.concat(dfs, ignore_index=True)\n",
    "    datasets.append(comb)\n",
    "#Concatenate all 4 dialects\n",
    "dfs = pd.concat(datasets, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7bd544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST THAT DIALECT IDENTIFICATION WORKS CORRECTLY\n",
    "tfidf = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.8, ngram_range=[1,3])\n",
    "dfs_x = dfs[\"Document\"]\n",
    "dfs_y = dfs[\"Label\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(dfs_x, dfs_y, test_size=0.2, random_state=4)\n",
    "x_train = tfidf.fit_transform(x_train)\n",
    "x_test = tfidf.transform(x_test)\n",
    "dialect_clf = MultinomialNB()\n",
    "#dialect_clf = SVC(kernel=\"sigmoid\")\n",
    "#dialect_clf = LinearSVC()\n",
    "#dialect_clf = RandomForestClassifier()\n",
    "dialect_clf.fit(x_train, y_train)\n",
    "y_pred = dialect_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2f8db7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.78%\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "#Accuracy, Feature importance, and Confusion Matrix\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(acc * 100))\n",
    "print(\"==========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06460a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "################ SENTIMENT ANALYSIS ##################\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b978714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Malay -> Indian -> Reference\n",
    "sentiment_comb = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "befa1965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Size:  5336 \n",
      "Head:\n",
      "      Rating                                             Review\n",
      "0  positive  Came here for the High Tea Great service espec...\n",
      "1  negative  5 stars for the service even though some of th...\n",
      "2  negative  Hi thank you for your service But i feel so so...\n",
      "3  negative  I have the worse buffer dinner ever so far The...\n",
      "4  positive  Thats are Known 5 Elmark  9H72   KDK  3 K14Y9 ...\n"
     ]
    }
   ],
   "source": [
    "#Read the Malay file\n",
    "sentiment_comb = []\n",
    "filepath = \"../Corpora/Sentiment-Malay/data_cleaned/GoogleReview_data_cleaned.csv\"\n",
    "df = pd.read_csv(filepath, sep=\",\", engine=\"python\", encoding=\"ISO-8859-1\", nrows=3000)\n",
    "#Drop unused columns\n",
    "df=df.drop([\"Author\", \"Restaurant\", \"Location\"], axis=1)\n",
    "#Remove rows that contain review of 3.0 as these contain inconsistent sentiment\n",
    "df=df[df[\"Rating\"]!=3.0]\n",
    "print(type(df))\n",
    "df.loc[df[\"Rating\"] == 4.0, \"Rating\"] = \"positive\"\n",
    "df.loc[df[\"Rating\"] == 5.0, \"Rating\"] = \"positive\"\n",
    "df.loc[df[\"Rating\"] == 2.0, \"Rating\"] = \"negative\"\n",
    "df.loc[df[\"Rating\"] == 1.0, \"Rating\"] = \"negative\"\n",
    "df = df.replace(to_replace=r'[^\\w\\s]', value='', regex=True)\n",
    "print(\"Size: \",df.size, \"\\nHead:\\n\", df.head())\n",
    "sentiment_comb.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1842e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size:  3740 \n",
      "Head:\n",
      "      Rating                                             Review\n",
      "0  negative  I bought this hair oil after viewing so many g...\n",
      "1  positive  Used This Mama Earth Newly Launched Onion Oil ...\n",
      "2  negative  So bad productMy hair falling increase too muc...\n",
      "3  negative  Product just smells similar to navarathna hair...\n",
      "4  positive  I have been trying different onion oil for my ...\n"
     ]
    }
   ],
   "source": [
    "#Read the Indian file\n",
    "sentiment_comb = []\n",
    "filepath = \"../Corpora/Sentiment-Indian/amazon_vfl_reviews.csv\"\n",
    "df = pd.read_csv(filepath, sep=\",\", engine=\"python\", encoding=\"ISO-8859-1\", nrows=2000)\n",
    "df=df.drop([\"asin\", \"name\", \"date\"], axis=1)\n",
    "df=df.rename(columns={\"rating\":\"Rating\"})\n",
    "df=df.rename(columns={\"review\":\"Review\"})\n",
    "df=df[df[\"Rating\"]!=3.0]\n",
    "df.loc[df[\"Rating\"] == 4.0, \"Rating\"] = \"positive\"\n",
    "df.loc[df[\"Rating\"] == 5.0, \"Rating\"] = \"positive\"\n",
    "df.loc[df[\"Rating\"] == 2.0, \"Rating\"] = \"negative\"\n",
    "df.loc[df[\"Rating\"] == 1.0, \"Rating\"] = \"negative\"\n",
    "df = df.replace(to_replace=r'[^\\w\\s]', value='', regex=True)\n",
    "print(\"Size: \",df.size, \"\\nHead:\\n\", df.head())\n",
    "sentiment_comb.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54b5c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the Reference file\n",
    "sentiment_comb = []\n",
    "filepath = \"../Corpora/Sentiment-Reference/test.txt.txt\"\n",
    "df = pd.DataFrame()\n",
    "df[\"X\"] = pd.read_csv(filepath, sep=\"\\t\", engine=\"python\", encoding=\"ISO-8859-1\", nrows=5000)\n",
    "df[[\"Rating\", \"Review\"]] = df[\"X\"].str.split(\" \", n=1, expand=True)\n",
    "df.drop(\"X\", axis=1, inplace=True)\n",
    "df.loc[df[\"Rating\"] == \"__label__1\", \"Rating\"] = \"negative\"\n",
    "df.loc[df[\"Rating\"] == \"__label__2\", \"Rating\"] = \"positive\"\n",
    "df.head()\n",
    "sentiment_comb.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3096d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_comb = []\n",
    "x_test_comb = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e3619c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "#Malay\n",
    "y_test_comb = []\n",
    "x_test_comb = []\n",
    "tfidf = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.8, ngram_range=[1,3])\n",
    "dfs_x = df[\"Review\"]\n",
    "dfs_y = df[\"Rating\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(dfs_x, dfs_y, test_size=0.2, random_state=4)\n",
    "y_test_comb.append(y_test)\n",
    "x_test_comb.append(x_test)\n",
    "x_train = tfidf.fit_transform(x_train)\n",
    "x_test = tfidf.transform(x_test)\n",
    "malay_clf = MultinomialNB()\n",
    "#malay_clf = SVC(kernel=\"sigmoid\")\n",
    "#malay_clf = LinearSVC()\n",
    "#malay_clf = RandomForestClassifier()\n",
    "malay_clf.fit(x_train, y_train)\n",
    "#y_pred = malaya_clf.predict(x_test)\n",
    "#acc = accuracy_score(y_test, y_pred)\n",
    "#print(\"Accuracy: {:.2f}%\".format(acc * 100))\n",
    "y_test_comb.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "77b8a396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "#Indian\n",
    "y_test_comb = []\n",
    "x_test_comb = []\n",
    "tfidf = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.8, ngram_range=[1,3])\n",
    "dfs_x = df[\"Review\"]\n",
    "dfs_y = df[\"Rating\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(dfs_x, dfs_y, test_size=0.2, random_state=4)\n",
    "y_test_comb.append(y_test)\n",
    "x_test_comb.append(x_test)\n",
    "x_train = tfidf.fit_transform(x_train)\n",
    "x_test = tfidf.transform(x_test)\n",
    "indian_clf = MultinomialNB()\n",
    "#malay_clf = SVC(kernel=\"sigmoid\")\n",
    "#malay_clf = LinearSVC()\n",
    "#malay_clf = RandomForestClassifier()\n",
    "indian_clf.fit(x_train, y_train)\n",
    "#y_pred = malaya_clf.predict(x_test)\n",
    "#acc = accuracy_score(y_test, y_pred)\n",
    "#print(\"Accuracy: {:.2f}%\".format(acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f46a3035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "#Reference\n",
    "y_test_comb = []\n",
    "x_test_comb = []\n",
    "tfidf = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.8, ngram_range=[1,3])\n",
    "dfs_x = df[\"Review\"]\n",
    "dfs_y = df[\"Rating\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(dfs_x, dfs_y, test_size=0.2, random_state=4)\n",
    "y_test_comb.append(y_test)\n",
    "x_test_comb.append(x_test)\n",
    "x_train = tfidf.fit_transform(x_train)\n",
    "x_test = tfidf.transform(x_test)\n",
    "ref_clf = MultinomialNB()\n",
    "#malay_clf = SVC(kernel=\"sigmoid\")\n",
    "#malay_clf = LinearSVC()\n",
    "#malay_clf = RandomForestClassifier()\n",
    "ref_clf.fit(x_train, y_train)\n",
    "#y_pred = malaya_clf.predict(x_test)\n",
    "#acc = accuracy_score(y_test, y_pred)\n",
    "#print(\"Accuracy: {:.2f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e129ef86",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [70]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m((\u001b[43mx_test_comb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      2\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, ngram_range\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m      3\u001b[0m x_test_comb[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m tfidf\u001b[38;5;241m.\u001b[39mfit_transform(x_test_comb[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#print((x_test_comb).shape)\n",
    "#tfidf = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.8, ngram_range=[1,3])\n",
    "#x_test_comb[0] = tfidf.fit_transform(x_test_comb[0])\n",
    "#y_test_comb[0] = tfidf.transform(y_test_comb[0])\n",
    "#testpred = dialect_clf.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c12150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
